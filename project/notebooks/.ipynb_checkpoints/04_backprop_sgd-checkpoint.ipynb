{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def dsigmoid_from_activation(a):\n",
    "    # a = sigmoid(z) já calculado\n",
    "    return a * (1 - a)\n",
    "\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def drelu_from_preactivation(z):\n",
    "    return (z > 0).astype(float)\n",
    "\n",
    "def mse(y_pred, y_true):\n",
    "    return np.mean((y_pred - y_true) ** 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class CamadaDensa:\n",
    "    def __init__(self, n_inputs, n_neuronios, ativacao=\"relu\", seed=None):\n",
    "        rng = np.random.default_rng(seed)\n",
    "        # Inicialização simples (poderíamos melhorar depois)\n",
    "        self.W = rng.standard_normal((n_neuronios, n_inputs))\n",
    "        self.b = rng.standard_normal(n_neuronios)\n",
    "        self.ativacao = ativacao\n",
    "        # caches\n",
    "        self.x = None\n",
    "        self.z = None\n",
    "        self.a = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        self.x = x  # (n_inputs,)\n",
    "        self.z = self W @ x + self.b  # pre-ativação\n",
    "        if self.ativacao == \"relu\":\n",
    "            self.a = relu(self.z)\n",
    "        elif self.ativacao == \"sigmoid\":\n",
    "            self.a = sigmoid(self.z)\n",
    "        else:\n",
    "            self.a = self.z  # linear\n",
    "        return self.a\n",
    "\n",
    "    def backward(self, grad_a):\n",
    "        # grad_a = dL/da (vindo da frente)\n",
    "        if self.ativacao == \"relu\":\n",
    "            grad_z = grad_a * drelu_from_preactivation(self.z)\n",
    "        elif self.ativacao == \"sigmoid\":\n",
    "            grad_z = grad_a * dsigmoid_from_activation(self.a)\n",
    "        else:\n",
    "            grad_z = grad_a  # linear\n",
    "\n",
    "        # Gradientes de W e b\n",
    "        grad_W = np.outer(grad_z, self.x)     # (nout, nin)\n",
    "        grad_b = grad_z                        # (nout,)\n",
    "        # Gradiente que vai para trás (para a camada anterior)\n",
    "        grad_x = self.W.T @ grad_z             # (nin,)\n",
    "\n",
    "        return grad_x, grad_W, grad_b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, n_inputs, n_hidden, n_outputs, seed=None):\n",
    "        self.l1 = CamadaDensa(n_inputs, n_hidden, ativacao=\"relu\", seed=seed)\n",
    "        self.l2 = CamadaDensa(n_hidden, n_outputs, ativacao=\"sigmoid\", seed=None if seed is None else seed+1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        a1 = self.l1.forward(x)\n",
    "        a2 = self.l2.forward(a1)\n",
    "        return a2\n",
    "\n",
    "    def backward(self, y_pred, y_true):\n",
    "        # dL/dy_pred para MSE: 2*(y_pred - y_true)/n ; como n=dim saída, constante comum\n",
    "        # Vamos usar forma simples: grad = (y_pred - y_true) (constante 2/n embutida na taxa de aprendizado)\n",
    "        grad_out = (y_pred - y_true)  # dL/da2\n",
    "\n",
    "        # Camada de saída (sigmoid)\n",
    "        grad_a1, grad_W2, grad_b2 = self.l2.backward(grad_out)\n",
    "\n",
    "        # Camada oculta (ReLU)\n",
    "        _,     grad_W1, grad_b1 = self.l1.backward(grad_a1)\n",
    "\n",
    "        return grad_W1, grad_b1, grad_W2, grad_b2\n",
    "\n",
    "    def step(self, grads, lr=1e-2):\n",
    "        grad_W1, grad_b1, grad_W2, grad_b2 = grads\n",
    "        self.l1.W -= lr * grad_W1\n",
    "        self.l1.b -= lr * grad_b1\n",
    "        self.l2.W -= lr * grad_W2\n",
    "        self.l2.b -= lr * grad_b2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Exemplo: 3 entradas -> 5 ocultos -> 2 saídas\n",
    "rng = np.random.default_rng(42)\n",
    "rede = MLP(n_inputs=3, n_hidden=5, n_outputs=2, seed=0)\n",
    "\n",
    "x = np.array([0.5, -1.2, 3.3])\n",
    "y_true = np.array([1.0, 0.0])\n",
    "\n",
    "lr = 0.05\n",
    "epochs = 1000\n",
    "\n",
    "for ep in range(1, epochs+1):\n",
    "    y_pred = rede.forward(x)\n",
    "    loss = mse(y_pred, y_true)\n",
    "\n",
    "    grads = rede.backward(y_pred, y_true)\n",
    "    rede.step(grads, lr=lr)\n",
    "\n",
    "    if ep % 100 == 0 or ep == 1:\n",
    "        print(f\"época {ep:4d} | loss {loss:.6f} | y_pred {y_pred}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Pequeno dataset sintético\n",
    "X = rng.standard_normal((8, 3))\n",
    "Y = np.stack([\n",
    "    (X[:,0] + 0.5*X[:,2] > 0).astype(float),   # alvo 1 binário\n",
    "    (X[:,1] < 0).astype(float)                 # alvo 2 binário\n",
    "], axis=1)\n",
    "\n",
    "rede = MLP(n_inputs=3, n_hidden=6, n_outputs=2, seed=123)\n",
    "lr = 0.05\n",
    "epochs = 1000\n",
    "\n",
    "for ep in range(1, epochs+1):\n",
    "    # embaralhar\n",
    "    idx = rng.permutation(len(X))\n",
    "    Xb, Yb = X[idx], Y[idx]\n",
    "\n",
    "    # forward em todo o lote, acumulando gradientes simples (SGD batch inteiro)\n",
    "    sum_loss = 0.0\n",
    "    # acumular gradientes\n",
    "    acc_gW1 = np.zeros_like(rede.l1.W)\n",
    "    acc_gb1 = np.zeros_like(rede.l1.b)\n",
    "    acc_gW2 = np.zeros_like(rede.l2.W)\n",
    "    acc_gb2 = np.zeros_like(rede.l2.b)\n",
    "\n",
    "    for xi, yi in zip(Xb, Yb):\n",
    "        yp = rede.forward(xi)\n",
    "        sum_loss += mse(yp, yi)\n",
    "        gW1, gb1, gW2, gb2 = rede.backward(yp, yi)\n",
    "        acc_gW1 += gW1; acc_gb1 += gb1\n",
    "        acc_gW2 += gW2; acc_gb2 += gb2\n",
    "\n",
    "    # média do gradiente\n",
    "    n = len(Xb)\n",
    "    rede.step((acc_gW1/n, acc_gb1/n, acc_gW2/n, acc_gb2/n), lr=lr)\n",
    "\n",
    "    if ep % 100 == 0 or ep == 1:\n",
    "        print(f\"época {ep:4d} | loss médio {sum_loss/n:.6f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
